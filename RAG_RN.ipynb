{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIWQ0SHJHIGfhnn06H7Nl9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaveenHashira/GAN/blob/main/RAG_RN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsVSLyCLnax-",
        "outputId": "ca578988-8487-4ba8-8c40-8086cee9bb3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/313.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq langchain-core langgraph langchain-groq pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "import os\n",
        "from typing import TypedDict, List, Dict, Any, Optional,Annotated\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "from pymongo import MongoClient\n",
        "import json"
      ],
      "metadata": {
        "id": "CHt61X_0npON"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the state\n",
        "class State(TypedDict):\n",
        "  server_uri:str\n",
        "  question:str\n",
        "  metadata:Dict\n",
        "  filter:Dict\n",
        "  context:List[str]\n",
        "  memory:Annotated[list,add_messages]"
      ],
      "metadata": {
        "id": "jLtjCwM-nxUL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize llm\n",
        "#model = ChatGroq(\n",
        "    #model_name=\"llama3-8b-8192\",\n",
        "    #temperature=6,\n",
        "    #api_key=groq_api_key\n",
        "#)"
      ],
      "metadata": {
        "id": "zy9Cf0DJx9Z-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing state\n",
        "def update_state_initial(mongodbServer: str,user_query: Optional[str] = None) -> State:\n",
        "  if user_query is not None:\n",
        "        state['question'] = user_query"
      ],
      "metadata": {
        "id": "2fuBwUh70gEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting metadata from the user query\n",
        "def extract_metadata_update_state(state: State, user_query: Optional[str] = None) -> State:\n",
        "    \"\"\"\n",
        "    updates state['question'] with a new user query,\n",
        "    extracts relevant metadata (e.g., companies, names, time periods)\n",
        "    from the question using a language model, and updates state['metadata'].\n",
        "\n",
        "    Args:\n",
        "        state (State): The state object to update.\n",
        "        user_query (Optional[str]): New query to set as state['question'] (if provided).\n",
        "\n",
        "    Returns:\n",
        "        State: The updated state with extracted metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    if user_query is not None:\n",
        "        state['question'] = user_query\n",
        "\n",
        "    question = state.get('question', '')\n",
        "\n",
        "    if not question:\n",
        "        # Handle empty question  (e.g., skip extraction or raise warning)\n",
        "        state['metadata'] = {}\n",
        "        return state\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Extract relevant metadata (company names or user names, time periods) from the following questions:\n",
        "\n",
        "Q: What were the revenue growth rates for Apple and Amazon from 2019 to 2022?\n",
        "Metadata:\n",
        "- Companies: Apple, Amazon\n",
        "- Time period: 2019-2022\n",
        "\n",
        "Q: How did Naveen and Kavin perform financially in Q1 2023?\n",
        "Metadata:\n",
        "- Names: Naveen , Kavin\n",
        "- Time period: Q1 2023\n",
        "\n",
        "Q: Which companies led the smartphone market in 2021 and 2022?\n",
        "Metadata:\n",
        "- Companies: smartphone market leaders (e.g., Samsung, Apple)\n",
        "- Time period: 2021-2022\n",
        "\n",
        "Q: {question}\n",
        "Metadata:\n",
        "\"\"\"\n",
        "\n",
        "    # Calling the LLM\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Extract metadata from model response (assuming structured text like the examples)\n",
        "    metadata_str = response.content.strip()\n",
        "\n",
        "    # Improved parsing: Handle lines and split values more robustly\n",
        "    metadata = {}\n",
        "    current_key = None\n",
        "    for line in metadata_str.split('\\n'):\n",
        "        line = line.strip()\n",
        "        if line.startswith('-') and ':' in line:\n",
        "            key, value = line.split(':', 1)\n",
        "            key = key.strip('- ').strip()\n",
        "            values = [x.strip() for x in value.split(',') if x.strip()]\n",
        "            metadata[key] = values if len(values) > 1 else values[0] if values else ''\n",
        "            current_key = key\n",
        "        elif current_key and line:  # Handle multi-line values if needed\n",
        "            if isinstance(metadata[current_key], list):\n",
        "                metadata[current_key].append(line)\n",
        "            else:\n",
        "                metadata[current_key] += f\" {line}\"\n",
        "\n",
        "    # Update the state with the extracted metadata\n",
        "    state['metadata'] = metadata\n",
        "\n",
        "    return state"
      ],
      "metadata": {
        "id": "W5AKhTB4wE1C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_metadata_filter(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Generates a MongoDB filter from state['metadata'] (with Companies, Names, Time period),\n",
        "    updates state['filter'] with it, and returns the updated state.\n",
        "\n",
        "    Args:\n",
        "        state (State): The state object containing 'metadata' to build the filter from.\n",
        "\n",
        "    Returns:\n",
        "        State: The updated state with 'filter' populated.\n",
        "    \"\"\"\n",
        "    metadata = state.get('metadata', {})\n",
        "\n",
        "    # Collect all name/company values into one list\n",
        "    name_company_values = []\n",
        "    if 'Companies' in metadata and metadata['Companies']:\n",
        "        name_company_values.extend(metadata['Companies'])\n",
        "    if 'Names' in metadata and metadata['Names']:\n",
        "        name_company_values.extend(metadata['Names'])\n",
        "\n",
        "    time_period_value = metadata.get('Time period')\n",
        "\n",
        "    # Build filter for intersection (logical AND)\n",
        "    if name_company_values and time_period_value:\n",
        "        generated_filter = {\n",
        "            '$and': [\n",
        "                {\n",
        "                    '$or': [\n",
        "                        {'company': {'$in': name_company_values}},\n",
        "                        {'name': {'$in': name_company_values}}\n",
        "                    ]\n",
        "                },\n",
        "                {'time_period': time_period_value}\n",
        "            ]\n",
        "        }\n",
        "    elif name_company_values:\n",
        "        generated_filter = {\n",
        "            '$or': [\n",
        "                {'company': {'$in': name_company_values}},\n",
        "                {'name': {'$in': name_company_values}}\n",
        "            ]\n",
        "        }\n",
        "    elif time_period_value:\n",
        "        generated_filter = {'time_period': time_period_value}\n",
        "    else:\n",
        "        generated_filter = {}\n",
        "\n",
        "    # Update the state with the generated filter\n",
        "    state['filter'] = generated_filter\n",
        "\n",
        "    return state\n",
        "\n",
        "state = {\n",
        "    'metadata': {\n",
        "        'Companies': ['CompanyX', 'CompanyY'],\n",
        "        'Names': ['Alice', 'Bob'],\n",
        "        'Time period': '2023-Q2'\n",
        "    }\n",
        "}\n",
        "updated_state = generate_metadata_filter(state)\n",
        "print(updated_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgctsCXOHyy0",
        "outputId": "58378db9-16ea-4119-b454-8b16c412385c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'Companies': ['CompanyX', 'CompanyY'], 'Names': ['Alice', 'Bob'], 'Time period': '2023-Q2'}, 'filter': {'$and': [{'$or': [{'company': {'$in': ['CompanyX', 'CompanyY', 'Alice', 'Bob']}}, {'name': {'$in': ['CompanyX', 'CompanyY', 'Alice', 'Bob']}}]}, {'time_period': '2023-Q2'}]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# querying database and retrieving relevant data/info\n",
        "def query_all_databases_update_context(server_uri: str, state: State) -> State:\n",
        "    \"\"\"\n",
        "    Queries all MongoDB databases and collections using state['filter'],\n",
        "    updates state['context'] with stringified results (e.g. JSON),\n",
        "    and returns the updated state.\n",
        "    \"\"\"\n",
        "    client = MongoClient(server_uri)\n",
        "    query = state.get('filter', {})\n",
        "    db_names = client.list_database_names()\n",
        "    results = {}\n",
        "\n",
        "    for db_name in db_names:\n",
        "        db = client[db_name]\n",
        "        for coll_name in db.list_collection_names():\n",
        "            collection = db[coll_name]\n",
        "            matches = list(collection.find(query))\n",
        "            if matches:\n",
        "                # Store results in the dictionary\n",
        "                results.setdefault(db_name, {})[coll_name] = matches\n",
        "\n",
        "                # Convert documents to JSON strings and extend context\n",
        "                for doc in matches:\n",
        "                    # Use default=str to handle BSON types like ObjectId, datetime, etc.\n",
        "                    json_doc = json.dumps(doc, default=str)\n",
        "                    state['context'].append(json_doc)\n",
        "\n",
        "    client.close()\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "ibRBJlTBOpAE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve from vector store\n",
        "#def retrieve(state:State):\n",
        "  #retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "  #return {\"context\":retrieved_docs}"
      ],
      "metadata": {
        "id": "snh76kE4Pd4h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate answer\n",
        "def generate(state: dict):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert data analyst with skills in exploratory data analysis, statistical reasoning, and insight generation across any domain. Your goal is to analyze the provided data context in response to the user's question, delivering clear, actionable insights without assuming domain-specific knowledge.\n",
        "\n",
        "User Question: {state[\"question\"]}\n",
        "\n",
        "Retrieved Context (from MongoDB): {state[\"context\"]}\n",
        "\n",
        "Instructions:\n",
        "1. Understand the data: Summarize key elements, describe data types (numerical, categorical, temporal), structure (tables, lists, key-value pairs), and note any quality issues such as missing values, duplicates, or outliers.\n",
        "\n",
        "2. Align with the question: Interpret the user's question and break it down into sub-queries if needed (for example, trends, correlations, or comparisons).\n",
        "\n",
        "3. Perform analysis:\n",
        "   - Compute relevant statistics or perform comparisons as applicable, focusing strictly on what the question requires.\n",
        "   - Identify patterns, anomalies, or insights directly relevant to the question.\n",
        "   - Perform simple aggregations, groupings, or comparisons (e.g., by category or time period) when appropriate.\n",
        "   - Keep it efficient and avoid over-analysis.\n",
        "\n",
        "4. Generate insights: Provide 3-5 key findings with explanations, using simple language and quantifying results where possible (e.g., \"Sales increased by 20% in Q2\").\n",
        "\n",
        "5. Suggest visualizations: Recommend 1-2 simple visualizations (e.g., bar charts for distributions, line graphs for trends) and briefly describe what each would illustrate. Do not generate the charts.\n",
        "\n",
        "6. Handle limitations: If the data context is insufficient, unclear, or irrelevant, explain why and suggest what additional data is needed.\n",
        "\n",
        "7. Output format: Structure your response as:\n",
        "   - Summary of Data\n",
        "   - Key Insights (bullet points)\n",
        "   - Visualization Suggestions\n",
        "   - Recommendations or Next Steps\n",
        "\n",
        "Ensure your analysis is objective, evidence-based, and concise. Respond only with the analysis output.\n",
        "\"\"\"\n",
        "    # calling the llm\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    response = model.invoke(messages)\n",
        "    return {\"answer\": response.content}\n"
      ],
      "metadata": {
        "id": "BsKl-gFHP9VJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWdx2CYtlK1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}