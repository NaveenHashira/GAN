{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vXajUfzJUIm-"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraires\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "image_size = 28*28\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "epochs = 50\n",
        "sample_dir = 'samples'"
      ],
      "metadata": {
        "id": "r0_xWJaQyeka"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sec2_nQL0mk5",
        "outputId": "4c6d9bd8-c421-4ce5-f842-ad172cbeb1ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class generator(nn.Module):\n",
        "  def __init__(self, latent_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    self.generator = nn.Sequential(\n",
        "        nn.Linear(latent_dimension, 128*7*7),\n",
        "        nn.BatchNorm1d(128*7*7),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Unflatten(1, (128, 7, 7)),\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self,z):\n",
        "    x = self.generator(z)\n",
        "    return x #.view(-1,1,28,28)\n",
        "\n"
      ],
      "metadata": {
        "id": "x6hKxweX04pn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lol():\n",
        "  latent_dim = 100\n",
        "  batch_size = 2\n",
        "\n",
        "  z = torch.randn(batch_size, latent_dim)\n",
        "  model = generator(100)\n",
        "  output = model(z)\n",
        "\n",
        "  return output.shape\n",
        "\n",
        "print(lol())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO8AcUp0-dnL",
        "outputId": "2d539fb4-0033-43cb-c1fb-f0b8d5858f15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3),       # → (batch,32,26,26)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),           # → (batch,32,13,13)\n",
        "\n",
        "            nn.Conv2d(32, 64, 3),      # → (batch,64,11,11)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),           # → (batch,64,5,5)\n",
        "\n",
        "            nn.Flatten(),              # → (batch,64*5*5)\n",
        "            nn.Linear(64 * 5 * 5, 1),\n",
        "            nn.Sigmoid()               # output in [0,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "OF1Sulpt1wq5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lol():\n",
        "  x = torch.randn(3,1,28,28)\n",
        "  model = discriminator()\n",
        "  out = model(x)\n",
        "  return out\n",
        "\n",
        "print(lol())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjs498K4CzrI",
        "outputId": "d450465e-ad00-459b-c7ac-efc71dd44857"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4977],\n",
            "        [0.4202],\n",
            "        [0.3982]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "\n",
        "def label_as_zero(_):\n",
        "    return torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "mnist = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        "    target_transform=label_as_zero\n",
        "    )"
      ],
      "metadata": {
        "id": "EkYXYg2IppPZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "3g31-IOR1yAo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = generator(100).to(device)\n",
        "discriminator = discriminator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "8HdWkd6EFZ8u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "# 3. Inspect shapes\n",
        "print(images.shape)  # torch.Size([64, 1, 28, 28])\n",
        "print(labels.shape)  # torch.Size([64])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C06mR4Wp_ADi",
        "outputId": "3bf46a66-3e5e-4da1-97ff-6b606205b1e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dir = 'samples'\n",
        "# Create the samples directory if it doesn't exist\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (real_imgs, _) in enumerate(dataloader):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        fake_imgs = generator(z)\n",
        "\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        d_real = discriminator(real_imgs)\n",
        "        d_fake = discriminator(fake_imgs.detach())\n",
        "\n",
        "        loss_real = criterion(d_real, real_labels)\n",
        "        loss_fake = criterion(d_fake, fake_labels)\n",
        "        d_loss = loss_real + loss_fake\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        fake_imgs = generator(z)\n",
        "        g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] Batch {i}/{len(dataloader)} \\\n",
        "                  Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save samples every epoch\n",
        "    vutils.save_image(fake_imgs[:64], os.path.join(sample_dir, f\"epoch_{epoch+1}.png\"), normalize=True, nrow=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OIS9GPTJyV_6",
        "outputId": "eb2c85b0-ae48-432f-ed28-43c665a61608"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Batch 0/469                   Loss D: 0.0011, Loss G: 6.9295\n",
            "Epoch [1/50] Batch 200/469                   Loss D: 0.0008, Loss G: 7.1587\n",
            "Epoch [1/50] Batch 400/469                   Loss D: 0.0007, Loss G: 7.3310\n",
            "Epoch [2/50] Batch 0/469                   Loss D: 0.0006, Loss G: 7.4018\n",
            "Epoch [2/50] Batch 200/469                   Loss D: 0.0005, Loss G: 7.5875\n",
            "Epoch [2/50] Batch 400/469                   Loss D: 0.0007, Loss G: 7.7710\n",
            "Epoch [3/50] Batch 0/469                   Loss D: 0.0005, Loss G: 7.8144\n",
            "Epoch [3/50] Batch 200/469                   Loss D: 0.0004, Loss G: 7.9753\n",
            "Epoch [3/50] Batch 400/469                   Loss D: 0.0003, Loss G: 8.1224\n",
            "Epoch [4/50] Batch 0/469                   Loss D: 0.0003, Loss G: 8.1787\n",
            "Epoch [4/50] Batch 200/469                   Loss D: 0.0003, Loss G: 8.3218\n",
            "Epoch [4/50] Batch 400/469                   Loss D: 0.0006, Loss G: 8.4875\n",
            "Epoch [5/50] Batch 0/469                   Loss D: 0.0002, Loss G: 8.5182\n",
            "Epoch [5/50] Batch 200/469                   Loss D: 0.0002, Loss G: 8.6525\n",
            "Epoch [5/50] Batch 400/469                   Loss D: 0.0002, Loss G: 8.7875\n",
            "Epoch [6/50] Batch 0/469                   Loss D: 0.0001, Loss G: 8.8457\n",
            "Epoch [6/50] Batch 200/469                   Loss D: 0.0001, Loss G: 8.9701\n",
            "Epoch [6/50] Batch 400/469                   Loss D: 0.0001, Loss G: 9.1131\n",
            "Epoch [7/50] Batch 0/469                   Loss D: 0.0001, Loss G: 9.1546\n",
            "Epoch [7/50] Batch 200/469                   Loss D: 0.0001, Loss G: 9.2825\n",
            "Epoch [7/50] Batch 400/469                   Loss D: 0.0001, Loss G: 9.4064\n",
            "Epoch [8/50] Batch 0/469                   Loss D: 0.0001, Loss G: 9.4598\n",
            "Epoch [8/50] Batch 200/469                   Loss D: 0.0001, Loss G: 9.5837\n",
            "Epoch [8/50] Batch 400/469                   Loss D: 0.0001, Loss G: 9.6826\n",
            "Epoch [9/50] Batch 0/469                   Loss D: 0.0001, Loss G: 9.7326\n",
            "Epoch [9/50] Batch 200/469                   Loss D: 0.0001, Loss G: 9.8580\n",
            "Epoch [9/50] Batch 400/469                   Loss D: 0.0001, Loss G: 9.9914\n",
            "Epoch [10/50] Batch 0/469                   Loss D: 0.0000, Loss G: 10.0309\n",
            "Epoch [10/50] Batch 200/469                   Loss D: 0.0000, Loss G: 10.1524\n",
            "Epoch [10/50] Batch 400/469                   Loss D: 0.0001, Loss G: 10.2719\n",
            "Epoch [11/50] Batch 0/469                   Loss D: 0.0000, Loss G: 10.3045\n",
            "Epoch [11/50] Batch 200/469                   Loss D: 0.0000, Loss G: 10.4314\n",
            "Epoch [11/50] Batch 400/469                   Loss D: 0.0000, Loss G: 10.5590\n",
            "Epoch [12/50] Batch 0/469                   Loss D: 0.0000, Loss G: 10.5940\n",
            "Epoch [12/50] Batch 200/469                   Loss D: 0.0000, Loss G: 10.7111\n",
            "Epoch [12/50] Batch 400/469                   Loss D: 0.0000, Loss G: 10.8247\n",
            "Epoch [13/50] Batch 0/469                   Loss D: 0.0000, Loss G: 10.8648\n",
            "Epoch [13/50] Batch 200/469                   Loss D: 0.0000, Loss G: 10.9674\n",
            "Epoch [13/50] Batch 400/469                   Loss D: 0.0000, Loss G: 11.0955\n",
            "Epoch [14/50] Batch 0/469                   Loss D: 0.0000, Loss G: 11.1380\n",
            "Epoch [14/50] Batch 200/469                   Loss D: 0.0000, Loss G: 11.2413\n",
            "Epoch [14/50] Batch 400/469                   Loss D: 0.0000, Loss G: 11.3622\n",
            "Epoch [15/50] Batch 0/469                   Loss D: 0.0000, Loss G: 11.4049\n",
            "Epoch [15/50] Batch 200/469                   Loss D: 0.0000, Loss G: 11.5019\n",
            "Epoch [15/50] Batch 400/469                   Loss D: 0.0000, Loss G: 11.6293\n",
            "Epoch [16/50] Batch 0/469                   Loss D: 0.0000, Loss G: 11.6654\n",
            "Epoch [16/50] Batch 200/469                   Loss D: 0.0000, Loss G: 11.7792\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-938221211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-1304851591.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# TestScript.test_sequential_intermediary_types).  Cannot annotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# with Any as TorchScript expects a more precise type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96uNsKUJ10QC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}