{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vXajUfzJUIm-"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraires\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "image_size = 28*28\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "epochs = 50\n",
        "sample_dir = 'samples'"
      ],
      "metadata": {
        "id": "r0_xWJaQyeka"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sec2_nQL0mk5",
        "outputId": "6fb7ecd9-0f03-4c22-d26d-740653ad37e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class generator(nn.Module):\n",
        "  def __init__(self, latent_dimension):\n",
        "    super().__init__()\n",
        "\n",
        "    self.generator = nn.Sequential(\n",
        "    nn.Linear(latent_dimension, 128*7*7),\n",
        "    nn.BatchNorm1d(128*7*7),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Unflatten(1, (128, 7, 7)),\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 14x14\n",
        "    nn.ReLU(inplace=True),\n",
        "\n",
        "    nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),  # 28x28\n",
        "    nn.Tanh()\n",
        "  )\n",
        "\n",
        "\n",
        "  def forward(self,z):\n",
        "    x = self.generator(z)\n",
        "    return x #.view(-1,1,28,28)\n",
        "\n"
      ],
      "metadata": {
        "id": "x6hKxweX04pn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lol():\n",
        "  latent_dim = 100\n",
        "  batch_size = 2\n",
        "\n",
        "  z = torch.randn(batch_size, latent_dim)\n",
        "  model = generator(100)\n",
        "  output = model(z)\n",
        "\n",
        "  return output.shape\n",
        "\n",
        "print(lol())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO8AcUp0-dnL",
        "outputId": "3489d919-f9fc-4656-919f-7d7a906b5ddd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3),       # → (batch,32,26,26)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),           # → (batch,32,13,13)\n",
        "\n",
        "            nn.Conv2d(32, 64, 3),      # → (batch,64,11,11)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),           # → (batch,64,5,5)\n",
        "\n",
        "            nn.Flatten(),              # → (batch,64*5*5)\n",
        "            nn.Linear(64 * 5 * 5, 1),\n",
        "            nn.Sigmoid()               # output in [0,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "OF1Sulpt1wq5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lol():\n",
        "  x = torch.randn(3,1,28,28)\n",
        "  model = discriminator()\n",
        "  out = model(x)\n",
        "  return out\n",
        "\n",
        "print(lol())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjs498K4CzrI",
        "outputId": "d86278d6-bece-4a72-84b3-e51ece0b1326"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7468],\n",
            "        [0.7372],\n",
            "        [0.6452]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "\n",
        "def label_as_zero(_):\n",
        "    return torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "mnist = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        "    target_transform=label_as_zero\n",
        "    )"
      ],
      "metadata": {
        "id": "EkYXYg2IppPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3edfc5c-db2d-48d7-8e86-20a73bd58c4a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 505kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.97MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.45MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "3g31-IOR1yAo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = generator(100).to(device)\n",
        "discriminator = discriminator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n"
      ],
      "metadata": {
        "id": "8HdWkd6EFZ8u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "# 3. Inspect shapes\n",
        "print(images.shape)  # torch.Size([64, 1, 28, 28])\n",
        "print(labels.shape)  # torch.Size([64])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C06mR4Wp_ADi",
        "outputId": "b4dc82ca-e628-4551-8e6d-6a77dcb18e60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  sample_dir = 'samples'\n",
        "  # Create the samples directory if it doesn't exist\n",
        "  if not os.path.exists(sample_dir):\n",
        "      os.makedirs(sample_dir)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      for i, (real_imgs, _) in enumerate(dataloader):\n",
        "          real_imgs = real_imgs.to(device)\n",
        "          batch_size = real_imgs.size(0)\n",
        "\n",
        "          # === Train Discriminator ===\n",
        "          z = torch.randn(batch_size, latent_dim).to(device)\n",
        "          fake_imgs = generator(z)\n",
        "\n",
        "          real_labels = torch.ones(batch_size, 1).to(device)\n",
        "          fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "          d_real = discriminator(real_imgs)\n",
        "          d_fake = discriminator(fake_imgs.detach())\n",
        "\n",
        "          loss_real = criterion(d_real, real_labels)\n",
        "          loss_fake = criterion(d_fake, fake_labels)\n",
        "          d_loss = loss_real + loss_fake\n",
        "\n",
        "          optimizer_D.zero_grad()\n",
        "          d_loss.backward()\n",
        "          optimizer_D.step()\n",
        "\n",
        "          # === Train Generator ===\n",
        "          z = torch.randn(batch_size, latent_dim).to(device)\n",
        "          fake_imgs = generator(z)\n",
        "          g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "          g_loss.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          if i % 200 == 0:\n",
        "              print(f\"Epoch [{epoch+1}/{epochs}] Batch {i}/{len(dataloader)} \\\n",
        "                    Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "      # Save samples every epoch\n",
        "      vutils.save_image(fake_imgs[:64], os.path.join(sample_dir, f\"epoch_{epoch+1}.png\"), normalize=True, nrow=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIS9GPTJyV_6",
        "outputId": "abbae274-2c4a-4099-9eb0-549767e4847a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Batch 0/469                   Loss D: 1.4323, Loss G: 0.6299\n",
            "Epoch [1/50] Batch 200/469                   Loss D: 1.1915, Loss G: 0.9551\n",
            "Epoch [1/50] Batch 400/469                   Loss D: 0.9758, Loss G: 1.0197\n",
            "Epoch [2/50] Batch 0/469                   Loss D: 0.9095, Loss G: 1.1110\n",
            "Epoch [2/50] Batch 200/469                   Loss D: 1.4417, Loss G: 1.0101\n",
            "Epoch [2/50] Batch 400/469                   Loss D: 0.8814, Loss G: 1.3829\n",
            "Epoch [3/50] Batch 0/469                   Loss D: 0.6941, Loss G: 1.5561\n",
            "Epoch [3/50] Batch 200/469                   Loss D: 0.6881, Loss G: 1.4825\n",
            "Epoch [3/50] Batch 400/469                   Loss D: 1.1357, Loss G: 1.0846\n",
            "Epoch [4/50] Batch 0/469                   Loss D: 1.0399, Loss G: 1.1272\n",
            "Epoch [4/50] Batch 200/469                   Loss D: 1.0105, Loss G: 1.0298\n",
            "Epoch [4/50] Batch 400/469                   Loss D: 1.0768, Loss G: 1.0633\n",
            "Epoch [5/50] Batch 0/469                   Loss D: 0.9572, Loss G: 1.2006\n",
            "Epoch [5/50] Batch 200/469                   Loss D: 0.8029, Loss G: 1.1316\n",
            "Epoch [5/50] Batch 400/469                   Loss D: 0.8530, Loss G: 1.2957\n",
            "Epoch [6/50] Batch 0/469                   Loss D: 0.8982, Loss G: 1.2926\n",
            "Epoch [6/50] Batch 200/469                   Loss D: 0.9890, Loss G: 1.3362\n",
            "Epoch [6/50] Batch 400/469                   Loss D: 0.8981, Loss G: 1.4240\n",
            "Epoch [7/50] Batch 0/469                   Loss D: 0.8663, Loss G: 1.1656\n",
            "Epoch [7/50] Batch 200/469                   Loss D: 0.8068, Loss G: 1.3632\n",
            "Epoch [7/50] Batch 400/469                   Loss D: 0.9331, Loss G: 1.2500\n",
            "Epoch [8/50] Batch 0/469                   Loss D: 0.8247, Loss G: 1.4102\n",
            "Epoch [8/50] Batch 200/469                   Loss D: 0.7971, Loss G: 1.2045\n",
            "Epoch [8/50] Batch 400/469                   Loss D: 0.9331, Loss G: 1.0841\n",
            "Epoch [9/50] Batch 0/469                   Loss D: 0.8460, Loss G: 1.4165\n",
            "Epoch [9/50] Batch 200/469                   Loss D: 0.7531, Loss G: 1.5387\n",
            "Epoch [9/50] Batch 400/469                   Loss D: 0.9968, Loss G: 1.2576\n",
            "Epoch [10/50] Batch 0/469                   Loss D: 0.9104, Loss G: 1.4114\n",
            "Epoch [10/50] Batch 200/469                   Loss D: 0.9510, Loss G: 1.2055\n",
            "Epoch [10/50] Batch 400/469                   Loss D: 0.8137, Loss G: 1.3070\n",
            "Epoch [11/50] Batch 0/469                   Loss D: 0.7732, Loss G: 1.5746\n",
            "Epoch [11/50] Batch 200/469                   Loss D: 0.7808, Loss G: 1.3476\n",
            "Epoch [11/50] Batch 400/469                   Loss D: 0.7277, Loss G: 1.6758\n",
            "Epoch [12/50] Batch 0/469                   Loss D: 0.6959, Loss G: 1.5679\n",
            "Epoch [12/50] Batch 200/469                   Loss D: 0.7067, Loss G: 1.7118\n",
            "Epoch [12/50] Batch 400/469                   Loss D: 0.7963, Loss G: 1.3653\n",
            "Epoch [13/50] Batch 0/469                   Loss D: 0.8428, Loss G: 1.2310\n",
            "Epoch [13/50] Batch 200/469                   Loss D: 0.8802, Loss G: 1.4871\n",
            "Epoch [13/50] Batch 400/469                   Loss D: 0.8010, Loss G: 1.6070\n",
            "Epoch [14/50] Batch 0/469                   Loss D: 0.7759, Loss G: 1.6152\n",
            "Epoch [14/50] Batch 200/469                   Loss D: 0.8593, Loss G: 1.2732\n",
            "Epoch [14/50] Batch 400/469                   Loss D: 0.8146, Loss G: 1.1319\n",
            "Epoch [15/50] Batch 0/469                   Loss D: 0.8746, Loss G: 1.6044\n",
            "Epoch [15/50] Batch 200/469                   Loss D: 0.7632, Loss G: 1.5756\n",
            "Epoch [15/50] Batch 400/469                   Loss D: 0.7808, Loss G: 1.3076\n",
            "Epoch [16/50] Batch 0/469                   Loss D: 0.8071, Loss G: 1.4119\n",
            "Epoch [16/50] Batch 200/469                   Loss D: 0.7754, Loss G: 1.6219\n",
            "Epoch [16/50] Batch 400/469                   Loss D: 0.9284, Loss G: 1.4023\n",
            "Epoch [17/50] Batch 0/469                   Loss D: 0.9133, Loss G: 1.3135\n",
            "Epoch [17/50] Batch 200/469                   Loss D: 0.7826, Loss G: 1.3673\n",
            "Epoch [17/50] Batch 400/469                   Loss D: 0.8229, Loss G: 1.3602\n",
            "Epoch [18/50] Batch 0/469                   Loss D: 0.8439, Loss G: 1.6591\n",
            "Epoch [18/50] Batch 200/469                   Loss D: 0.6917, Loss G: 1.4832\n",
            "Epoch [18/50] Batch 400/469                   Loss D: 0.8435, Loss G: 1.5167\n",
            "Epoch [19/50] Batch 0/469                   Loss D: 0.9453, Loss G: 1.5258\n",
            "Epoch [19/50] Batch 200/469                   Loss D: 0.7996, Loss G: 1.6869\n",
            "Epoch [19/50] Batch 400/469                   Loss D: 0.8332, Loss G: 1.5416\n",
            "Epoch [20/50] Batch 0/469                   Loss D: 0.7327, Loss G: 1.6355\n",
            "Epoch [20/50] Batch 200/469                   Loss D: 0.8106, Loss G: 1.7358\n",
            "Epoch [20/50] Batch 400/469                   Loss D: 0.8546, Loss G: 2.0006\n",
            "Epoch [21/50] Batch 0/469                   Loss D: 0.6807, Loss G: 1.4529\n",
            "Epoch [21/50] Batch 200/469                   Loss D: 0.8159, Loss G: 1.6200\n",
            "Epoch [21/50] Batch 400/469                   Loss D: 0.8122, Loss G: 1.3778\n",
            "Epoch [22/50] Batch 0/469                   Loss D: 0.7386, Loss G: 1.8969\n",
            "Epoch [22/50] Batch 200/469                   Loss D: 0.7206, Loss G: 1.7366\n",
            "Epoch [22/50] Batch 400/469                   Loss D: 0.7079, Loss G: 1.8462\n",
            "Epoch [23/50] Batch 0/469                   Loss D: 0.8774, Loss G: 1.5904\n",
            "Epoch [23/50] Batch 200/469                   Loss D: 0.7805, Loss G: 1.5366\n",
            "Epoch [23/50] Batch 400/469                   Loss D: 0.6765, Loss G: 1.8547\n",
            "Epoch [24/50] Batch 0/469                   Loss D: 0.8995, Loss G: 1.5432\n",
            "Epoch [24/50] Batch 200/469                   Loss D: 0.6431, Loss G: 2.1029\n",
            "Epoch [24/50] Batch 400/469                   Loss D: 0.6890, Loss G: 1.4583\n",
            "Epoch [25/50] Batch 0/469                   Loss D: 0.6710, Loss G: 2.4389\n",
            "Epoch [25/50] Batch 200/469                   Loss D: 0.8324, Loss G: 1.3749\n",
            "Epoch [25/50] Batch 400/469                   Loss D: 0.6727, Loss G: 1.3953\n",
            "Epoch [26/50] Batch 0/469                   Loss D: 0.7377, Loss G: 1.5982\n",
            "Epoch [26/50] Batch 200/469                   Loss D: 0.7645, Loss G: 1.8278\n",
            "Epoch [26/50] Batch 400/469                   Loss D: 0.8603, Loss G: 1.8221\n",
            "Epoch [27/50] Batch 0/469                   Loss D: 0.7438, Loss G: 1.8247\n",
            "Epoch [27/50] Batch 200/469                   Loss D: 0.8514, Loss G: 1.8130\n",
            "Epoch [27/50] Batch 400/469                   Loss D: 0.7800, Loss G: 1.3869\n",
            "Epoch [28/50] Batch 0/469                   Loss D: 0.7779, Loss G: 1.7897\n",
            "Epoch [28/50] Batch 200/469                   Loss D: 0.7917, Loss G: 1.3817\n",
            "Epoch [28/50] Batch 400/469                   Loss D: 0.8972, Loss G: 1.4341\n",
            "Epoch [29/50] Batch 0/469                   Loss D: 0.7371, Loss G: 1.4776\n",
            "Epoch [29/50] Batch 200/469                   Loss D: 0.7149, Loss G: 1.3913\n",
            "Epoch [29/50] Batch 400/469                   Loss D: 0.9101, Loss G: 1.1823\n",
            "Epoch [30/50] Batch 0/469                   Loss D: 0.5920, Loss G: 1.5398\n",
            "Epoch [30/50] Batch 200/469                   Loss D: 0.8100, Loss G: 1.8350\n",
            "Epoch [30/50] Batch 400/469                   Loss D: 0.7845, Loss G: 1.6802\n",
            "Epoch [31/50] Batch 0/469                   Loss D: 0.8135, Loss G: 1.8444\n",
            "Epoch [31/50] Batch 200/469                   Loss D: 0.6781, Loss G: 1.5755\n",
            "Epoch [31/50] Batch 400/469                   Loss D: 0.6496, Loss G: 1.4892\n",
            "Epoch [32/50] Batch 0/469                   Loss D: 0.6614, Loss G: 2.0774\n",
            "Epoch [32/50] Batch 200/469                   Loss D: 1.0025, Loss G: 1.4638\n",
            "Epoch [32/50] Batch 400/469                   Loss D: 0.9084, Loss G: 1.5103\n",
            "Epoch [33/50] Batch 0/469                   Loss D: 0.8544, Loss G: 1.6983\n",
            "Epoch [33/50] Batch 200/469                   Loss D: 0.8325, Loss G: 1.8075\n",
            "Epoch [33/50] Batch 400/469                   Loss D: 0.8204, Loss G: 2.0421\n",
            "Epoch [34/50] Batch 0/469                   Loss D: 0.7358, Loss G: 2.0180\n",
            "Epoch [34/50] Batch 200/469                   Loss D: 0.7321, Loss G: 1.7175\n",
            "Epoch [34/50] Batch 400/469                   Loss D: 0.6994, Loss G: 1.9794\n",
            "Epoch [35/50] Batch 0/469                   Loss D: 0.6525, Loss G: 1.8849\n",
            "Epoch [35/50] Batch 200/469                   Loss D: 0.8957, Loss G: 1.5066\n",
            "Epoch [35/50] Batch 400/469                   Loss D: 0.8722, Loss G: 1.7155\n",
            "Epoch [36/50] Batch 0/469                   Loss D: 0.6955, Loss G: 1.7307\n",
            "Epoch [36/50] Batch 200/469                   Loss D: 0.9295, Loss G: 1.2161\n",
            "Epoch [36/50] Batch 400/469                   Loss D: 0.7632, Loss G: 1.6364\n",
            "Epoch [37/50] Batch 0/469                   Loss D: 0.7026, Loss G: 2.0076\n",
            "Epoch [37/50] Batch 200/469                   Loss D: 0.7912, Loss G: 2.1779\n",
            "Epoch [37/50] Batch 400/469                   Loss D: 0.7072, Loss G: 1.6685\n",
            "Epoch [38/50] Batch 0/469                   Loss D: 0.7696, Loss G: 1.5369\n",
            "Epoch [38/50] Batch 200/469                   Loss D: 1.0240, Loss G: 1.7182\n",
            "Epoch [38/50] Batch 400/469                   Loss D: 0.8110, Loss G: 1.7804\n",
            "Epoch [39/50] Batch 0/469                   Loss D: 0.5368, Loss G: 1.9561\n",
            "Epoch [39/50] Batch 200/469                   Loss D: 0.7288, Loss G: 1.7907\n",
            "Epoch [39/50] Batch 400/469                   Loss D: 0.9450, Loss G: 1.8040\n",
            "Epoch [40/50] Batch 0/469                   Loss D: 0.7876, Loss G: 1.5290\n",
            "Epoch [40/50] Batch 200/469                   Loss D: 0.8394, Loss G: 1.3435\n",
            "Epoch [40/50] Batch 400/469                   Loss D: 0.9771, Loss G: 1.9913\n",
            "Epoch [41/50] Batch 0/469                   Loss D: 0.7505, Loss G: 2.2681\n",
            "Epoch [41/50] Batch 200/469                   Loss D: 0.7505, Loss G: 1.6966\n",
            "Epoch [41/50] Batch 400/469                   Loss D: 0.8726, Loss G: 1.8332\n",
            "Epoch [42/50] Batch 0/469                   Loss D: 0.9610, Loss G: 1.7975\n",
            "Epoch [42/50] Batch 200/469                   Loss D: 0.7477, Loss G: 2.2006\n",
            "Epoch [42/50] Batch 400/469                   Loss D: 0.9786, Loss G: 2.0369\n",
            "Epoch [43/50] Batch 0/469                   Loss D: 0.8094, Loss G: 2.2080\n",
            "Epoch [43/50] Batch 200/469                   Loss D: 0.8282, Loss G: 1.6485\n",
            "Epoch [43/50] Batch 400/469                   Loss D: 0.8939, Loss G: 1.9311\n",
            "Epoch [44/50] Batch 0/469                   Loss D: 0.8651, Loss G: 1.6300\n",
            "Epoch [44/50] Batch 200/469                   Loss D: 0.7631, Loss G: 2.0442\n",
            "Epoch [44/50] Batch 400/469                   Loss D: 0.7984, Loss G: 2.1128\n",
            "Epoch [45/50] Batch 0/469                   Loss D: 0.7089, Loss G: 1.5365\n",
            "Epoch [45/50] Batch 200/469                   Loss D: 0.7734, Loss G: 2.0342\n",
            "Epoch [45/50] Batch 400/469                   Loss D: 0.7658, Loss G: 1.7858\n",
            "Epoch [46/50] Batch 0/469                   Loss D: 0.6815, Loss G: 1.7058\n",
            "Epoch [46/50] Batch 200/469                   Loss D: 0.7574, Loss G: 1.6619\n",
            "Epoch [46/50] Batch 400/469                   Loss D: 0.7458, Loss G: 1.9446\n",
            "Epoch [47/50] Batch 0/469                   Loss D: 0.8076, Loss G: 1.6445\n",
            "Epoch [47/50] Batch 200/469                   Loss D: 0.7780, Loss G: 2.0668\n",
            "Epoch [47/50] Batch 400/469                   Loss D: 0.7474, Loss G: 2.4255\n",
            "Epoch [48/50] Batch 0/469                   Loss D: 0.8043, Loss G: 1.3284\n",
            "Epoch [48/50] Batch 200/469                   Loss D: 0.8450, Loss G: 1.8120\n",
            "Epoch [48/50] Batch 400/469                   Loss D: 0.7196, Loss G: 1.7375\n",
            "Epoch [49/50] Batch 0/469                   Loss D: 0.5886, Loss G: 1.9814\n",
            "Epoch [49/50] Batch 200/469                   Loss D: 0.7037, Loss G: 1.9003\n",
            "Epoch [49/50] Batch 400/469                   Loss D: 0.7134, Loss G: 1.5988\n",
            "Epoch [50/50] Batch 0/469                   Loss D: 0.7222, Loss G: 1.7413\n",
            "Epoch [50/50] Batch 200/469                   Loss D: 0.8811, Loss G: 2.1499\n",
            "Epoch [50/50] Batch 400/469                   Loss D: 0.7020, Loss G: 2.0521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.eval()\n",
        "z = torch.randn(1, latent_dim).to(device)\n",
        "with torch.no_grad():\n",
        "    fake_img = generator(z)"
      ],
      "metadata": {
        "id": "96uNsKUJ10QC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi6jIMAeCCsG",
        "outputId": "4691c89f-ee47-4f67-908d-c27c008f6b42"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9992,\n",
              "           -0.9990, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -0.9429, -0.4556, -0.0044, -0.1661,\n",
              "           -0.9840, -0.9990, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -0.9929, -0.5811,  0.0586,  0.0888, -0.3421,\n",
              "           -0.9986, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -0.9515, -0.5604, -0.0248,  0.1039, -0.3291, -0.9155,\n",
              "           -0.9981, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9992, -0.3898,  0.0344,  0.0808, -0.1678, -0.9430, -0.9989,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9984, -0.0358,  0.0913,  0.0701, -0.3264, -0.9905, -1.0000,\n",
              "           -1.0000, -1.0000, -0.9999, -1.0000, -1.0000, -1.0000, -0.9999,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9871, -0.2915, -0.0559, -0.4480, -0.9951, -1.0000, -1.0000,\n",
              "           -1.0000, -0.9998, -0.9987, -0.9906, -0.9693, -0.6599, -0.7311,\n",
              "           -0.8610, -0.9360, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9997, -0.7309, -0.7531, -0.9979, -1.0000, -1.0000, -0.9996,\n",
              "           -0.8948, -0.5811, -0.2823, -0.2014, -0.1454,  0.0112,  0.0111,\n",
              "           -0.0853, -0.2496, -0.9529, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9985, -0.9353, -0.8176, -0.9092, -0.8545, -0.4296, -0.2482,\n",
              "            0.0442,  0.0208,  0.0234,  0.0554,  0.0428, -0.0938, -0.1054,\n",
              "            0.0517, -0.0013, -0.4164, -0.9376, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9879, -0.2428,  0.1007,  0.1114,  0.0609,  0.0771,  0.0714,\n",
              "            0.0692,  0.0272,  0.0139, -0.1880, -0.7031, -0.8764, -0.3424,\n",
              "           -0.1272,  0.0561,  0.0465, -0.3012, -0.9795, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9274, -0.1526,  0.0872,  0.0668,  0.0087,  0.0037, -0.0439,\n",
              "           -0.0621, -0.0848, -0.5083, -0.9763, -0.9994, -0.9944, -0.9829,\n",
              "           -0.9464, -0.0469,  0.0393, -0.0919, -0.9787, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9989, -0.7053, -0.1166, -0.3484, -0.2573, -0.3745, -0.5814,\n",
              "           -0.9443, -0.9620, -0.9516, -0.9161, -0.9942, -0.7865, -0.6925,\n",
              "           -0.5419, -0.0482, -0.0935, -0.5828, -0.9978, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -0.9998, -1.0000, -1.0000, -1.0000, -0.9996, -0.9994, -0.9864,\n",
              "           -0.9911, -0.5485, -0.1385, -0.1579, -0.2059,  0.0514,  0.0164,\n",
              "           -0.1597, -0.6023, -0.7079, -0.9680, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9925, -0.7679,\n",
              "           -0.4659,  0.0096,  0.0122,  0.0058, -0.0108, -0.1122, -0.2225,\n",
              "           -0.7344, -0.9996, -0.9990, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9999, -1.0000, -0.9992,\n",
              "           -0.9268, -0.9994, -0.9994, -0.9379, -0.6316, -0.2470, -0.1043,\n",
              "            0.0287,  0.0262,  0.0312, -0.0074, -0.5372, -0.9576, -0.9117,\n",
              "           -0.9998, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9991, -0.6032,\n",
              "           -0.1191, -0.2952, -0.2585, -0.0770,  0.0825,  0.0704,  0.0400,\n",
              "            0.0264, -0.0754, -0.5075, -0.8842, -0.9994, -1.0000, -0.9997,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9988, -0.4866,\n",
              "            0.0583,  0.0510,  0.0792,  0.0678,  0.0323, -0.0850, -0.2890,\n",
              "           -0.4761, -0.9011, -0.9886, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9722,\n",
              "           -0.5976, -0.4350, -0.3797, -0.4454, -0.7413, -0.9293, -0.9676,\n",
              "           -0.9998, -0.9994, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
              "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4t8VC-bIDFk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}